{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d11db18-afa3-4920-869a-5676753bf75a",
   "metadata": {},
   "source": [
    "ASSIGMENT:-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b9e1b-37e5-41f9-8b16-9256a8189ec3",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66713686-8ba8-4473-853d-2cbc22ecf050",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a simple and effective machine learning technique used for both classification and regression tasks. It is a lazy learning algorithm, meaning it does not explicitly train a model but makes predictions based on the dataset at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece4326f-021f-414b-acd7-b6a3ac2f6d89",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4607e933-6508-44e9-9e4e-3bbf3390a21d",
   "metadata": {},
   "source": [
    "1. Small vs. Large Values of K\n",
    "Small Values of K:\n",
    "\n",
    "Lead to more sensitive models that rely on very close neighbors.\n",
    "\n",
    "Can capture local patterns effectively, but are prone to noise and overfitting.\n",
    "\n",
    "Example: \n",
    "ùêæ\n",
    "=\n",
    "1\n",
    " means the model considers only the nearest neighbor, which can overfit to the training data.\n",
    "\n",
    "Large Values of K:\n",
    "\n",
    "Lead to smoother decision boundaries by considering more neighbors.\n",
    "\n",
    "Are less sensitive to noise but may underfit and fail to capture local patterns.\n",
    "\n",
    "Example: Choosing \n",
    "ùêæ\n",
    " close to the size of the dataset averages predictions across many points, which may oversimplify the relationships.\n",
    "\n",
    "2. Guidelines for Choosing K\n",
    "Start with an odd value for \n",
    "ùêæ\n",
    " (in classification) to avoid ties in voting when dealing with binary class labels.\n",
    "\n",
    "A commonly used heuristic is: \n",
    "ùêæ\n",
    "=\n",
    "ùëõ\n",
    ", where \n",
    "ùëõ\n",
    " is the number of data points in the training set.\n",
    "\n",
    "For regression tasks, consider testing a range of \n",
    "ùêæ\n",
    " values and evaluating performance on a validation set.\n",
    "\n",
    "3. Use Cross-Validation\n",
    "Perform k-fold cross-validation on the training dataset to test different values of \n",
    "ùêæ\n",
    ".\n",
    "\n",
    "Choose the \n",
    "ùêæ\n",
    " that minimizes the error metric (e.g., accuracy for classification or mean squared error for regression).\n",
    "\n",
    "4. Domain Knowledge\n",
    "If possible, use domain knowledge to inform the choice of \n",
    "ùêæ\n",
    ". For example, if you're working with highly local data (e.g., geographical), a smaller \n",
    "ùêæ\n",
    " may be more appropriate.\n",
    "\n",
    "5. Visualization\n",
    "In some cases, you can visualize the performance (e.g., accuracy or error rate) for different values of \n",
    "ùêæ\n",
    " and pick the value where the performance stabilizes or peaks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81403783-b678-48b6-bf26-b9e2d70dde6a",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f1b98b-bd37-4fd3-95f3-1d9534dbb5a3",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm can be applied to both classification and regression tasks, but its behavior differs based on the objective. Here's a comparison:\n",
    "\n",
    "Aspect\tKNN Classifier\tKNN Regressor\n",
    "Goal\tPredict a class label (e.g., cat, dog)\tPredict a numerical value (e.g., house price)\n",
    "Prediction Method\tAssigns the class based on majority voting among the \n",
    "ùêæ\n",
    " nearest neighbors\tCalculates the average (or weighted average) of the values of the \n",
    "ùêæ\n",
    " nearest neighbors\n",
    "Output\tDiscrete categories or labels\tContinuous numerical values\n",
    "Example\tDetermine if an email is spam or not\tPredict the temperature based on historical data\n",
    "Evaluation Metrics\tAccuracy, Precision, Recall, F1 Score\tMean Squared Error (MSE), Root Mean Squared Error (RMSE)\n",
    "Key Difference\n",
    "Classifier: It predicts the most frequent class among \n",
    "ùêæ\n",
    " neighbors.\n",
    "\n",
    "Regressor: It predicts the average (or weighted average) of numerical values of the \n",
    "ùêæ\n",
    " neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e81ea-405f-47ed-b6f2-6dd3cd83b727",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841983d-3135-48c9-84ff-fefb4d74e395",
   "metadata": {},
   "source": [
    "The performance of the K-Nearest Neighbors (KNN) algorithm is measured using different evaluation metrics depending on whether the task is classification or regression. Here's how it can be evaluated:\n",
    "\n",
    "Performance Metrics for KNN Classifier\n",
    "Accuracy:\n",
    "\n",
    "Proportion of correctly classified samples out of the total number of samples.\n",
    "\n",
    "Precision:\n",
    "\n",
    "Measures how many of the predicted positive instances are actually positive.\n",
    "\n",
    "Recall (Sensitivity):\n",
    "\n",
    "Measures how many of the actual positive instances were correctly predicted.\n",
    "\n",
    "F1 Score:\n",
    "\n",
    "Combines precision and recall into a single metric using the harmonic mean.\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "Evaluates the model's performance in terms of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "Performance Metrics for KNN Regressor\n",
    "Mean Squared Error (MSE):\n",
    "\n",
    "Measures the average squared difference between actual and predicted values.\n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "\n",
    "Measures the average absolute difference between actual and predicted values.\n",
    "\n",
    "Root Mean Squared Error (RMSE):\n",
    "\n",
    "Square root of MSE, providing an error measure in the same units as the target variable.\n",
    "\n",
    "Coefficient of Determination (\n",
    "ùëÖ\n",
    "2\n",
    "):\n",
    "\n",
    "Indicates how well the predictions explain the variability in the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b4811-0d4d-4c06-aa8a-62ee6a53dc6d",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4497f9-beb7-4128-94de-9976ad2592a1",
   "metadata": {},
   "source": [
    "What is the Curse of Dimensionality?\n",
    "Distance Becomes Less Meaningful:\n",
    "\n",
    "In high-dimensional spaces, all data points tend to become equidistant. This is because as the number of dimensions increases, the differences in distance between points become less pronounced.\n",
    "\n",
    "KNN relies on distance metrics (e.g., Euclidean distance) to find the nearest neighbors, but when distances lose their significance, the algorithm struggles to identify meaningful neighbors.\n",
    "\n",
    "Sparsity of Data:\n",
    "\n",
    "High-dimensional data is inherently sparse, meaning the points are scattered far apart. This sparsity makes it harder for KNN to group similar points effectively and compute accurate predictions.\n",
    "\n",
    "Computational Complexity:\n",
    "\n",
    "The number of dimensions increases the amount of computation required to calculate distances between points, making KNN slow and resource-intensive for high-dimensional datasets.\n",
    "\n",
    "Overfitting Risks:\n",
    "\n",
    "As dimensions increase, the risk of overfitting grows because KNN may find irrelevant neighbors in feature spaces that do not contribute meaningfully to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19754e39-c4b1-4076-b813-e72f736aa973",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a637e8-a5cf-40d4-912e-6bf6fb3686e6",
   "metadata": {},
   "source": [
    "1. Imputation Before Applying KNN\n",
    "Before running the KNN algorithm, you can fill in (impute) the missing values in the dataset using one of the following methods:\n",
    "\n",
    "Mean/Median Imputation: Replace missing values with the mean or median of the corresponding feature.\n",
    "\n",
    "Mode Imputation: For categorical variables, replace missing values with the most frequent category (mode).\n",
    "\n",
    "Domain-Specific Rules: Use domain knowledge to estimate missing values when applicable.\n",
    "\n",
    "2. KNN-Based Imputation\n",
    "A more sophisticated approach is to use a KNN imputer, which estimates missing values based on the values of the \n",
    "ùêæ\n",
    "-nearest neighbors.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Identify the \n",
    "ùêæ\n",
    "-nearest neighbors of the data point with missing values using other features.\n",
    "\n",
    "Use the values of these neighbors to impute the missing value.\n",
    "\n",
    "For numeric data: Take the average (or weighted average).\n",
    "\n",
    "For categorical data: Use majority voting.\n",
    "\n",
    "Advantages: Maintains relationships in the data and avoids bias introduced by simpler imputation techniques.\n",
    "\n",
    "Libraries: Many libraries, like scikit-learn in Python, provide built-in KNN imputation functionalities.\n",
    "\n",
    "3. Removing Rows or Columns\n",
    "When to Use: If a large portion of the dataset is complete and the missing data is minimal:\n",
    "\n",
    "Remove rows with missing values if their number is small and their removal won't impact the results.\n",
    "\n",
    "Drop columns with excessive missing values if they are not critical for analysis.\n",
    "\n",
    "4. Use Algorithms that Handle Missing Data\n",
    "Some algorithms (like Decision Trees or Random Forests) can handle missing values directly. In such cases, preprocessing might not be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4adb7-787e-4e45-8511-d4aea472188c",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f23148-0448-44ea-8d52-c84a0c446b50",
   "metadata": {},
   "source": [
    "Performance Comparison: KNN Classifier vs KNN Regressor\n",
    "Aspect\tKNN Classifier\tKNN Regressor\n",
    "Output Type\tPredicts discrete class labels (e.g., \"cat,\" \"dog\").\tPredicts continuous numerical values (e.g., house prices).\n",
    "Decision Logic\tUses majority voting among K nearest neighbors to determine the most frequent class.\tCalculates the mean (or weighted average) of the target values of the K nearest neighbors.\n",
    "Use Cases\tIdeal for problems with distinct categories like spam detection, image classification, or customer segmentation.\tSuitable for problems requiring numerical predictions like stock price forecasting or energy consumption estimation.\n",
    "Evaluation Metrics\tMetrics include accuracy, precision, recall, F1 score, and confusion matrix.\tMetrics include Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R¬≤ score.\n",
    "Sensitivity to K Value\tA small K can lead to overfitting; large K smooths decision boundaries.\tA small K may cause unstable predictions; large K averages neighbors' values, reducing sensitivity to outliers.\n",
    "Performance with Noise\tNoise can mislead class predictions in classification tasks, leading to incorrect votes.\tOutliers can influence the average value and make predictions less accurate.\n",
    "Computational Complexity\tBoth are computationally intensive for large datasets, as distance calculations are required for all data points.\tSimilar complexity, but regression may require additional operations for averaging values.\n",
    "Which is Better for Which Type of Problem?\n",
    "KNN Classifier:\n",
    "\n",
    "Best suited for problems where the output belongs to predefined categories (e.g., binary or multi-class labels).\n",
    "\n",
    "Example: Determining if an email is spam (Yes/No) or recognizing handwritten digits.\n",
    "\n",
    "KNN Regressor:\n",
    "\n",
    "Better for problems where the goal is to predict a continuous value.\n",
    "\n",
    "Example: Forecasting house prices or estimating temperatures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9cf4a7-f8ef-476b-93ac-064d812e9782",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c388c708-3c3a-475f-a0a2-cb493eb9b8f0",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm has several strengths and weaknesses when applied to both classification and regression tasks. Here's a detailed breakdown and ways to address the challenges:\n",
    "\n",
    "Strengths of KNN\n",
    "Simplicity:\n",
    "\n",
    "Easy to understand and implement, requiring no complex model training.\n",
    "\n",
    "Flexibility:\n",
    "\n",
    "Works well for both classification and regression tasks.\n",
    "\n",
    "Non-Parametric:\n",
    "\n",
    "Makes no assumptions about the underlying data distribution, making it suitable for diverse datasets.\n",
    "\n",
    "Adaptability:\n",
    "\n",
    "Can capture complex relationships in data with the right distance metric and preprocessing.\n",
    "\n",
    "Local Information:\n",
    "\n",
    "Relies on neighbors, which helps incorporate local patterns effectively.\n",
    "\n",
    "Weaknesses of KNN\n",
    "Computational Cost:\n",
    "\n",
    "Requires distance calculations for all data points, which can be expensive for large datasets.\n",
    "\n",
    "Curse of Dimensionality:\n",
    "\n",
    "Performance deteriorates as the number of features (dimensions) increases, because all points tend to become equidistant.\n",
    "\n",
    "Choice of \n",
    "ùêæ\n",
    ":\n",
    "\n",
    "Selecting the optimal value for \n",
    "ùêæ\n",
    " can be challenging and significantly impacts performance.\n",
    "\n",
    "Sensitive to Noise:\n",
    "\n",
    "Noisy data or irrelevant features can disrupt the algorithm's predictions, especially with small values of \n",
    "ùêæ\n",
    ".\n",
    "\n",
    "Feature Scaling:\n",
    "\n",
    "KNN is sensitive to the scale of features, as larger-scaled features dominate the distance calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bd0dde-3791-43f4-a9f0-eb2f21565519",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a40dc8-0a92-409b-94a9-4ee41a970fdc",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e97dc21-e53e-48e2-8cf2-99f7e6fd667e",
   "metadata": {},
   "source": [
    "Feature scaling plays a critical role in the performance of the K-Nearest Neighbors (KNN) algorithm. Here‚Äôs why it's important:\n",
    "\n",
    "Role of Feature Scaling\n",
    "Ensures Fair Distance Calculation:\n",
    "\n",
    "KNN relies on distance metrics (e.g., Euclidean or Manhattan distance) to identify the nearest neighbors.\n",
    "\n",
    "If features are on different scales, those with larger numerical ranges dominate the distance calculation, biasing the results.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a dataset with two features: Age (0‚Äì100 years) and Income (0‚Äì1,000,000).\n",
    "\n",
    "Without scaling, differences in income will outweigh differences in age, making age nearly irrelevant in neighbor selection.\n",
    "\n",
    "Improves Accuracy:\n",
    "\n",
    "Scaling ensures that all features contribute equally to the distance metric, leading to more accurate neighbor identification and predictions.\n",
    "\n",
    "Prevents Bias:\n",
    "\n",
    "Features with larger ranges or higher magnitudes can lead to biased predictions if scaling is not applied.\n",
    "\n",
    "Common Scaling Techniques\n",
    "Normalization:\n",
    "\n",
    "Rescales values to fit within a range of 0 to 1.\n",
    "\n",
    "Formula: $$ x' = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}} $$\n",
    "\n",
    "Standardization:\n",
    "\n",
    "Rescales values to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Formula: $$ x' = \\frac{x - \\mu}{\\sigma} $$\n",
    "\n",
    "ùúá\n",
    ": Mean of the feature.\n",
    "\n",
    "ùúé\n",
    ": Standard deviation of the feature.\n",
    "\n",
    "Log Transformation:\n",
    "\n",
    "Applies logarithmic scaling for features with extreme ranges or skewed distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2de66-c607-434c-bc34-bf18f9f4b0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
