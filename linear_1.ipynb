{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da4baf06-8aac-49c9-8028-3678fd04f636",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784a6dff-531f-4e99-9f19-5652e5121182",
   "metadata": {},
   "source": [
    "Simple linear regression is a statistical method that models the relationship between two variables by fitting a linear equation to observed data. One variable is considered the independent variable (predictor), and the other is the dependent variable (response).\n",
    "Equation: The equation for simple linear regression is:\n",
    "y=β0​+β1​x+ϵ\n",
    "\n",
    "\n",
    "\n",
    " Multiple linear regression is an extension of simple linear regression that models the relationship between one dependent variable and two or more independent variables.\n",
    "Equation: The equation for multiple linear regression is:\n",
    "y=β0​+β1​x1​+β2​x2​+…+βn​xn​+ϵ\n",
    "\n",
    "\n",
    "\n",
    "difference between both\n",
    "\n",
    "\n",
    "\n",
    "Simple Linear Regression: One independent variable, one dependent variable.\n",
    "Multiple Linear Regression: Two or more independent variables, one dependent variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdaa4b39-9b88-4079-a1aa-3f8bf30e58ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "916f11b4-4fb7-44a5-84b6-984268dfd745",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'weight': [23,56,67],\n",
    "                     'height' : [145, 145 ,135]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ec3ca8e-8322-4d93-b8e3-e0e9190fc691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   weight  height\n",
       "0      23     145\n",
       "1      56     145\n",
       "2      67     135"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data # example of single_linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "621ace68-f56b-4fe3-ada6-4db8bba66f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'weight': [23,56,67],\n",
    "                     'height' : [145, 145 ,135],\n",
    "                    'age': [20,30,50]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd6d40b3-2bf5-4156-9c8a-4b3ae440f59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>145</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>145</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>135</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   weight  height  age\n",
       "0      23     145   20\n",
       "1      56     145   30\n",
       "2      67     135   50"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data # example of multi_linear_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0153688a-466d-4060-bb89-67624902b9a7",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f3490c-86f2-4586-ad74-43fdc3d20640",
   "metadata": {},
   "source": [
    "Assumptions of Linear Regression\n",
    "Linearity: The relationship between the independent and dependent variables should be linear.\n",
    "Independence: The residuals (errors) should be independent. This means that the residuals of one observation should not be correlated with the residuals of another.\n",
    "Homoscedasticity: The residuals should have constant variance at every level of the independent variable.\n",
    "Normality: The residuals of the model should be normally distributed.\n",
    "Checking the Assumptions\n",
    "Linearity:\n",
    "Scatter Plot: Plot the independent variable against the dependent variable. If the points form a pattern that resembles a straight line, the linearity assumption is likely met.\n",
    "Residual Plot: Plot the residuals against the predicted values. If there is no clear pattern, linearity is assumed.\n",
    "Independence:\n",
    "Durbin-Watson Test: This statistical test can be used to detect the presence of autocorrelation in the residuals.\n",
    "Residual Time Series Plot: For time series data, plot residuals against time to check for patterns.\n",
    "Homoscedasticity:\n",
    "Residuals vs. Fitted Values Plot: Plot the residuals against the fitted values. If the residuals are spread equally across the range of fitted values, homoscedasticity is assumed.\n",
    "Breusch-Pagan Test: This test can be used to detect heteroscedasticity.\n",
    "Normality:\n",
    "Q-Q Plot: Plot the quantiles of the residuals against the quantiles of a normal distribution. If the points lie approximately along a straight line, the residuals are normally distributed.\n",
    "Shapiro-Wilk Test: This test can be used to statistically assess the normality of the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54b23c-5633-4cdb-98f1-d98a1a6a4b77",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136e46b-c9ba-48af-a844-6a0d6d543097",
   "metadata": {},
   "source": [
    "In a linear regression model, the slope and intercept are key components of the equation:\n",
    "y=mx+b\n",
    "where:\n",
    "\n",
    "( y ) is the dependent variable (the outcome you’re predicting),\n",
    "( x ) is the independent variable (the predictor),\n",
    "( m ) is the slope of the line,\n",
    "( b ) is the y-intercept.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Slope (m): This represents the change in the dependent variable ( y ) for a one-unit change in the independent variable ( x ). In other words, it shows how much ( y ) changes when ( x ) increases by one unit.\n",
    "Intercept (b): This is the value of ( y ) when ( x ) is zero. It represents the starting point or baseline value of the dependent variable.\n",
    "\n",
    "Real-World Example:\n",
    "Let’s say you’re analyzing the relationship between the number of hours studied (independent variable, ( x )) and the score on a test (dependent variable, ( y )).\n",
    "After performing linear regression, you get the following equation:\n",
    "Score=5×Hours Studied+50\n",
    "\n",
    "Slope (5): For every additional hour studied, the test score increases by 5 points.\n",
    "Intercept (50): If a student doesn’t study at all (0 hours), their predicted test score would be 50."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4f5a1-f9b2-4315-b211-820e8d37ca72",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2311c8-27d9-456a-a7a1-58f196a9755d",
   "metadata": {},
   "source": [
    "Concept of Gradient Descent\n",
    "Objective: The primary goal of gradient descent is to find the minimum value of a cost function. This cost function measures the difference between the predicted values and the actual values.\n",
    "Gradient: The gradient is a vector of partial derivatives of the cost function with respect to the model parameters. It points in the direction of the steepest increase of the function.\n",
    "Descent: By moving in the opposite direction of the gradient (i.e., descending), we can iteratively adjust the model parameters to reduce the cost function.\n",
    "Learning Rate: This is a hyperparameter that determines the size of the steps taken towards the minimum. A smaller learning rate means smaller steps, which can lead to more precise convergence but may take longer. A larger learning rate means larger steps, which can speed up the process but may overshoot the minimum.\n",
    "How Gradient Descent is Used in Machine Learning\n",
    "Initialization: Start with initial values for the model parameters, often set randomly.\n",
    "Compute Gradient: Calculate the gradient of the cost function with respect to each parameter.\n",
    "Update Parameters: Adjust the parameters in the opposite direction of the gradient by a factor proportional to the learning rate.\n",
    "Iterate: Repeat the process until the cost function converges to a minimum value, or until a predefined number of iterations is reached.\n",
    "Types of Gradient Descent\n",
    "Batch Gradient Descent: Uses the entire dataset to compute the gradient at each step. This can be computationally expensive for large datasets.\n",
    "Stochastic Gradient Descent (SGD): Uses a single data point to compute the gradient at each step. This introduces noise but can lead to faster convergence.\n",
    "Mini-batch Gradient Descent: Uses a small subset of the dataset to compute the gradient at each step. This balances the efficiency of batch gradient descent and the speed of SGD.\n",
    "Applications in Machine Learning\n",
    "Linear Regression: Gradient descent is used to find the optimal coefficients that minimize the mean squared error between predicted and actual values.\n",
    "Neural Networks: It is used to optimize the weights and biases of the network to minimize the loss function.\n",
    "Logistic Regression: Gradient descent helps in finding the optimal parameters that minimize the cross-entropy loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173cfe5e-3942-4fa4-ab6a-3a75996d2c78",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700c4c75-6916-4ce3-9a3d-09a8f687a1b1",
   "metadata": {},
   "source": [
    "Multiple Linear Regression\n",
    "Multiple linear regression (MLR) is a statistical technique used to model the relationship between one dependent variable and two or more independent variables. The general form of the multiple linear regression equation is:\n",
    "y=β0​+β1​x1​+β2​x2​+…+βn​xn​+ϵ\n",
    "where:\n",
    "\n",
    "( y ) is the dependent variable.\n",
    "( \\beta_0 ) is the intercept.\n",
    "( \\beta_1, \\beta_2, \\ldots, \\beta_n ) are the coefficients for the independent variables ( x_1, x_2, \\ldots, x_n ).\n",
    "( \\epsilon ) is the error term.\n",
    "\n",
    "Simple Linear Regression\n",
    "Simple linear regression is a simpler form of regression analysis that models the relationship between a single dependent variable and a single independent variable. The equation for simple linear regression is:\n",
    "y=β0​+β1​x+ϵ\n",
    "where:\n",
    "\n",
    "( y ) is the dependent variable.\n",
    "( \\beta_0 ) is the intercept.\n",
    "( \\beta_1 ) is the coefficient for the independent variable ( x ).\n",
    "( \\epsilon ) is the error term.\n",
    "\n",
    "Key Differences\n",
    "\n",
    "\n",
    "Number of Independent Variables:\n",
    "\n",
    "Simple Linear Regression: Involves only one independent variable.\n",
    "Multiple Linear Regression: Involves two or more independent variables.\n",
    "\n",
    "\n",
    "\n",
    "Complexity:\n",
    "\n",
    "Simple Linear Regression: Easier to interpret and visualize since it deals with a single predictor.\n",
    "Multiple Linear Regression: More complex as it considers multiple predictors, making it suitable for modeling more intricate relationships.\n",
    "\n",
    "\n",
    "\n",
    "Equation Form:\n",
    "\n",
    "Simple Linear Regression: ( y = \\beta_0 + \\beta_1 x + \\epsilon )\n",
    "Multiple Linear Regression: ( y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n + \\epsilon )\n",
    "\n",
    "\n",
    "\n",
    "Use Cases:\n",
    "\n",
    "Simple Linear Regression: Used when the relationship between the dependent variable and a single independent variable is of interest.\n",
    "Multiple Linear Regression: Used when the outcome is influenced by multiple factors, allowing for a more comprehensive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb9e3c7-1e7c-481b-9155-ef0bb55fd6d6",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c97a09b-f34f-498c-a783-7cd14a9541b2",
   "metadata": {},
   "source": [
    "Multicollinearity occurs in multiple linear regression when two or more predictor variables are highly correlated, meaning they provide redundant information about the response variable. This can lead to several issues:\n",
    "\n",
    "Unstable Estimates: The coefficients of the correlated predictors can become highly sensitive to changes in the model, leading to large standard errors.\n",
    "Reduced Interpretability: It becomes difficult to determine the individual effect of each predictor on the response variable.\n",
    "Inflated Variance: The variance of the coefficient estimates increases, which can make the model less reliable.\n",
    "Detecting Multicollinearity\n",
    "Correlation Matrix: Check the correlation coefficients between pairs of predictor variables. High values (close to 1 or -1) indicate potential multicollinearity.\n",
    "Variance Inflation Factor (VIF): Calculate the VIF for each predictor. A VIF value greater than 10 is often considered indicative of high multicollinearity.\n",
    "Tolerance: Tolerance is the reciprocal of VIF. A tolerance value close to 0 indicates high multicollinearity.\n",
    "Condition Index: Values above 30 suggest multicollinearity.\n",
    "Addressing Multicollinearity\n",
    "Remove Highly Correlated Predictors: If two predictors are highly correlated, consider removing one of them from the model.\n",
    "Combine Predictors: Create a new predictor by combining the correlated variables, such as through principal component analysis (PCA).\n",
    "Regularization Techniques: Use techniques like Ridge Regression or Lasso Regression, which can handle multicollinearity by adding a penalty to the regression coefficients.\n",
    "Centering the Variables: Subtract the mean from each predictor to reduce multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e5e1f7-0a7e-4a0e-a9a9-6c3121ae223b",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb2db03-f611-4299-9432-7f780e876f6f",
   "metadata": {},
   "source": [
    "Polynomial regression is an extension of linear regression where the relationship between the independent variable (x) and the dependent variable (y) is modeled as an (n)th degree polynomial. This allows for capturing non-linear relationships between the variables.\n",
    "Polynomial Regression Model\n",
    "The polynomial regression model can be expressed as:\n",
    "y=β0​+β1​x+β2​x2+β3​x3+…+βn​xn+ϵ\n",
    "where:\n",
    "\n",
    "(y) is the dependent variable.\n",
    "(x) is the independent variable.\n",
    "(\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_n) are the coefficients.\n",
    "(\\epsilon) is the error term.\n",
    "\n",
    "Differences from Linear Regression\n",
    "\n",
    "\n",
    "Model Complexity:\n",
    "\n",
    "Linear Regression: Models the relationship as a straight line ((y = \\beta_0 + \\beta_1 x + \\epsilon)).\n",
    "Polynomial Regression: Models the relationship as a polynomial curve, allowing for more complex, non-linear relationships.\n",
    "\n",
    "\n",
    "\n",
    "Flexibility:\n",
    "\n",
    "Linear Regression: Limited to linear relationships.\n",
    "Polynomial Regression: Can fit a wider range of data patterns by adjusting the degree of the polynomial.\n",
    "\n",
    "\n",
    "\n",
    "Overfitting:\n",
    "\n",
    "Linear Regression: Less prone to overfitting due to its simplicity.\n",
    "Polynomial Regression: Higher-degree polynomials can lead to overfitting, especially with small datasets.\n",
    "\n",
    "\n",
    "\n",
    "Interpretability:\n",
    "\n",
    "Linear Regression: Easier to interpret as it involves a straight-line relationship.\n",
    "Polynomial Regression: More complex to interpret due to the higher-degree terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098eea55-3401-4f9e-a485-2b815a9808c5",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ffe550-0def-40c2-b2e6-e03475d1374e",
   "metadata": {},
   "source": [
    "Advantages of Polynomial Regression\n",
    "Flexibility: Polynomial regression can model a wide range of relationships, including non-linear ones. This makes it more versatile than linear regression, which can only model linear relationships1.\n",
    "Better Fit for Non-Linear Data: When the relationship between the independent and dependent variables is non-linear, polynomial regression can provide a better fit by capturing the curvature in the data2.\n",
    "Disadvantages of Polynomial Regression\n",
    "Complexity: Polynomial regression models are more complex and harder to interpret than linear regression models. The higher the degree of the polynomial, the more difficult it becomes to understand the relationship between variables1.\n",
    "Overfitting: Polynomial regression is prone to overfitting, especially with higher-degree polynomials. This means the model might fit the training data very well but perform poorly on new, unseen data1.\n",
    "Sensitivity to Outliers: Polynomial regression is more sensitive to outliers compared to linear regression. Outliers can significantly affect the model’s performance and accuracy1.\n",
    "When to Use Polynomial Regression\n",
    "You might prefer to use polynomial regression in the following situations:\n",
    "\n",
    "Non-Linear Relationships: When the data shows a clear non-linear pattern, polynomial regression can capture the underlying trend better than linear regression.\n",
    "Complex Patterns: If the relationship between variables is complex and cannot be adequately modeled by a straight line, polynomial regression can provide a more accurate representation.\n",
    "Improving Model Fit: When a linear model underfits the data, adding polynomial terms can improve the model’s fit and predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4240a94f-bd8f-4b5e-9184-dcb759bd1b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
